{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.170.5.172:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yarn'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('hdfs://zh:9000/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, SVMWithSGD, NaiveBayes\n",
    "from pyspark.mllib.tree import DecisionTree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/zh/spark_hadoop/spark/spark-8cc02205-6cb3-45ee-9bcb-c9b603111657/userFiles-be8137cf-2b25-4850-b8ec-9aae859338e1/org.slf4j_slf4j-api-1.7.7.jar', '/home/zh/spark_hadoop/spark/spark-8cc02205-6cb3-45ee-9bcb-c9b603111657/userFiles-be8137cf-2b25-4850-b8ec-9aae859338e1/org.scala-lang_scala-reflect-2.11.0.jar', '/home/zh/spark_hadoop/spark/spark-8cc02205-6cb3-45ee-9bcb-c9b603111657/userFiles-be8137cf-2b25-4850-b8ec-9aae859338e1/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar', '/home/zh/spark_hadoop/spark/spark-8cc02205-6cb3-45ee-9bcb-c9b603111657/userFiles-be8137cf-2b25-4850-b8ec-9aae859338e1/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar', '/home/zh/spark_hadoop/spark/spark-8cc02205-6cb3-45ee-9bcb-c9b603111657/userFiles-be8137cf-2b25-4850-b8ec-9aae859338e1/graphframes_graphframes-0.5.0-spark2.1-s_2.11.jar', '/home/zh/spark_hadoop/spark/spark-8cc02205-6cb3-45ee-9bcb-c9b603111657/userFiles-be8137cf-2b25-4850-b8ec-9aae859338e1', '', '/home/zh/spark_hadoop/spark/python/lib/py4j-0.10.4-src.zip', '/home/zh/spark_hadoop/spark/python', '/home/zh/Spark_ML_Experiment', '/home/zh/.ivy2/jars', '/usr/lib/python35.zip', '/usr/lib/python3.5', '/usr/lib/python3.5/plat-x86_64-linux-gnu', '/usr/lib/python3.5/lib-dynload', '/home/zh/.local/lib/python3.5/site-packages', '/usr/local/lib/python3.5/dist-packages', '/usr/lib/python3/dist-packages', '/home/zh/.local/lib/python3.5/site-packages/IPython/extensions', '/home/zh/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_header = sc.textFile('hdfs://zh:9000/stumbleUpon-evergreen/train_noheader.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"\\t\"4042\"\\t\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ball The predictions are part of an annual tradition for the Armonk New York based company which surveys its 3 000 researchers to find five ideas expected to take root in the next five years IBM the world s largest provider of computer services looks to Silicon Valley for input gleaning many ideas from its Almaden research center in San Jose California Holographic conversations projected from mobile phones lead this year s list The predictions also include air breathing batteries computer programs that can tell when and where traffic jams will take place environmental information generated by sensors in cars and phones and cities powered by the heat thrown off by computer servers These are all stretch goals and that s good said Paul Saffo managing director of foresight at the investment advisory firm Discern in San Francisco In an era when pessimism is the new black a little dose of technological optimism is not a bad thing For IBM it s not just idle speculation The company is one of the few big corporations investing in long range research projects and it counts on innovation to fuel growth Saffo said Not all of its predictions pan out though IBM was overly optimistic about the spread of speech technology for instance When the ideas do lead to products they can have broad implications for society as well as IBM s bottom line he said Research Spending They have continued to do research when all the other grand research organizations are gone said Saffo who is also a consulting associate professor at Stanford University IBM invested 5 8 billion in research and development last year 6 1 percent of revenue While that s down from about 10 percent in the early 1990s the company spends a bigger share on research than its computing rivals Hewlett Packard Co the top maker of personal computers spent 2 4 percent last year At Almaden scientists work on projects that don t always fit in with IBM s computer business The lab s research includes efforts to develop an electric car battery that runs 500 miles on one charge a filtration system for desalination and a program that shows changes in geographic data IBM rose 9 cents to 146 04 at 11 02 a m in New York Stock Exchange composite trading The stock had gained 11 percent this year before today Citizen Science The list is meant to give a window into the company s innovation engine said Josephine Cheng a vice president at IBM s Almaden lab All this demonstrates a real culture of innovation at IBM and willingness to devote itself to solving some of the world s biggest problems she said Many of the predictions are based on projects that IBM has in the works One of this year s ideas that sensors in cars wallets and personal devices will give scientists better data about the environment is an expansion of the company s citizen science initiative Earlier this year IBM teamed up with the California State Water Resources Control Board and the City of San Jose Environmental Services to help gather information about waterways Researchers from Almaden created an application that lets smartphone users snap photos of streams and creeks and report back on conditions The hope is that these casual observations will help local and state officials who don t have the resources to do the work themselves Traffic Predictors IBM also sees data helping shorten commutes in the next five years Computer programs will use algorithms and real time traffic information to predict which roads will have backups and how to avoid getting stuck Batteries may last 10 times longer in 2015 than today IBM says Rather than using the current lithium ion technology new models could rely on energy dense metals that only need to interact with the air to recharge Some electronic devices might ditch batteries altogether and use something similar to kinetic wristwatches which only need to be shaken to generate a charge The final prediction involves recycling the heat generated by computers and data centers Almost half of the power used by data centers is currently spent keeping the computers cool IBM scientists say it would be better to harness that heat to warm houses and offices In IBM s first list of predictions compiled at the end of 2006 researchers said instantaneous speech translation would become the norm That hasn t happened yet While some programs can quickly translate electronic documents and instant messages and other apps can perform limited speech translation there s nothing widely available that acts like the universal translator in Star Trek Second Life The company also predicted that online immersive environments such as Second Life would become more widespread While immersive video games are as popular as ever Second Life s growth has slowed Internet users are flocking instead to the more 2 D environments of Facebook Inc and Twitter Inc Meanwhile a 2007 prediction that mobile phones will act as a wallet ticket broker concierge bank and shopping assistant is coming true thanks to the explosion of smartphone applications Consumers can pay bills through their banking apps buy movie tickets and get instant feedback on potential purchases all with a few taps on their phones The nice thing about the list is that it provokes thought Saffo said If everything came true they wouldn t be doing their job To contact the reporter on this story Ryan Flinn in San Francisco at rflinn bloomberg net To contact the editor responsible for this story Tom Giles at tgiles5 bloomberg net by 2015, your mobile phone will project a 3-d image of anyone who calls and your laptop will be powered by kinetic energy. at least that\\\\u2019s what international business machines corp. sees in its crystal ball.\"\",\"\"url\"\":\"\"bloomberg news 2010 12 23 ibm predicts holographic calls air breathing batteries by 2015 html\"\"}\"\\t\"business\"\\t\"0.789131\"\\t\"2.055555556\"\\t\"0.676470588\"\\t\"0.205882353\"\\t\"0.047058824\"\\t\"0.023529412\"\\t\"0.443783175\"\\t\"0\"\\t\"0\"\\t\"0.09077381\"\\t\"0\"\\t\"0.245831182\"\\t\"0.003883495\"\\t\"1\"\\t\"1\"\\t\"24\"\\t\"0\"\\t\"5424\"\\t\"170\"\\t\"8\"\\t\"0.152941176\"\\t\"0.079129575\"\\t\"0\"',\n",
       " '\"http://www.popsci.com/technology/article/2012-07/electronic-futuristic-starting-gun-eliminates-advantages-races\"\\t\"8471\"\\t\"{\"\"title\"\":\"\"The Fully Electronic Futuristic Starting Gun That Eliminates Advantages in Races the fully electronic, futuristic starting gun that eliminates advantages in races the fully electronic, futuristic starting gun that eliminates advantages in races\"\",\"\"body\"\":\"\"And that can be carried on a plane without the hassle too The Omega E Gun Starting Pistol Omega It s easy to take for granted just how insanely close some Olympic races are and how much the minutiae of it all can matter The perfect example is the traditional starting gun Seems easy You pull a trigger and the race starts Boom What people don t consider When a conventional gun goes off the sound travels to the ears of the closest runner a fraction of a second sooner than the others That s just enough to matter and why the latest starting pistol has traded in the mechanical boom for orchestrated electronic noise Omega has been the watch company tasked as the official timekeeper of the Olympic Games since 1932 At the 2010 Vancouver games they debuted their new starting gun which is a far cry from the iconic revolvers associated with early games it s clearly electronic but still more than a button that s pressed to get the show rolling About as far away as you can get probably while still clearly being a starting gun Pull the trigger once and off the Olympians go If it s pressed twice consecutively it signals a false start Working through a speaker system is what eliminates any kind of advantage for athletes It s not a big advantage being close to a gun but the sound of the bullet traveling one meter every three milliseconds could contribute to a win Powder pistols have been connected to a speaker system before but even then runners could react to the sound of the real pistol firing rather than wait for the speaker sounds to reach them This year s setup will have speakers placed equidistant from runners forcing the sound to reach each competitor at exactly the same time It wouldn t be an enormous difference Omega Timing board member Peter H\\\\u00fcrzeler said in an email but when you think about reaction times being measured in tiny fractions of a second placing a speaker behind each lane has eliminated any sort of advantage for any athlete They all hear the start commands and signal at exactly the same moment There s also an ulterior reason for its look In a post September 11th world a gun on its way to a major event is going to raise more than a few TSA eyebrows even if it s a realistic looking fake Rather than deal with that the e gun can be transported while still maintaining the general look of a starting gun But there s still nothing like hearing a starting gun go off at the start of a race more than signaling the runners there s probably some Pavlovian response after more than a century of Olympic games that make people want to hear the real thing not a whiny electronic noise Everyone in the stands at home thankfully will still be getting that The sound is programmable and can be synthesized to sound like almost anything H\\\\u00fcrzeler says but we program it to sound like a pistol it s a way to use the best possible starting technology but to keep a rich tradition alive and that can be carried on a plane without the hassle, too technology,gadgets,london 2012,london olympics,olympics,omega,starting guns,summer olympics,timing,popular science,popsci\"\",\"\"url\"\":\"\"popsci technology article 2012 07 electronic futuristic starting gun eliminates advantages races\"\"}\"\\t\"recreation\"\\t\"0.574147\"\\t\"3.677966102\"\\t\"0.50802139\"\\t\"0.288770053\"\\t\"0.213903743\"\\t\"0.144385027\"\\t\"0.468648998\"\\t\"0\"\\t\"0\"\\t\"0.098707403\"\\t\"0\"\\t\"0.203489628\"\\t\"0.088652482\"\\t\"1\"\\t\"1\"\\t\"40\"\\t\"0\"\\t\"4973\"\\t\"187\"\\t\"9\"\\t\"0.181818182\"\\t\"0.125448029\"\\t\"1\"',\n",
       " '\"http://www.menshealth.com/health/flu-fighting-fruits?cm_mmc=Facebook-_-MensHealth-_-Content-Health-_-FightFluWithFruit\"\\t\"1164\"\\t\"{\"\"title\"\":\"\"Fruits that Fight the Flu fruits that fight the flu | cold & flu | men\\'s health\"\",\"\"body\"\":\"\"Apples The most popular source of antioxidants in our diet one apple has an antioxidant effect equivalent to 1 500 mg of vitamin C Apples are loaded with protective flavonoids which may prevent heart disease and cancer Next Papayas With 250 percent of the RDA of vitamin C a papaya can help kick a cold right out of your system The beta carotene and vitamins C and E in papayas reduce inflammation throughout the body lessening the effects of asthma Next Cranberries Cranberries have more antioxidants than other common fruits and veggies One serving has five times the amount in broccoli Cranberries are a natural probiotic enhancing good bacteria levels in the gut and protecting it from foodborne illnesses Next Grapefruit Loaded with vitamin C grapefruit also contains natural compounds called limonoids which can lower cholesterol The red varieties are a potent source of the cancer fighting substance lycopene Next Bananas One of the top food sources of vitamin B6 bananas help reduce fatigue depression stress and insomnia Bananas are high in magnesium which keeps bones strong and potassium which helps prevent heart disease and high blood pressure Next everything you need to know about cold and flu so you don\\\\u2019t get sick this season, at men\\\\u2019s health.com cold, flu, infection, sore throat, sneeze, immunity, germs, allergies, stay healthy, sick, contagious, medicines, cold medicine\"\",\"\"url\"\":\"\"menshealth health flu fighting fruits cm mmc Facebook Mens Health Content Health Fight Flu With Fruit\"\"}\"\\t\"health\"\\t\"0.996526\"\\t\"2.382882883\"\\t\"0.562015504\"\\t\"0.321705426\"\\t\"0.120155039\"\\t\"0.042635659\"\\t\"0.525448029\"\\t\"0\"\\t\"0\"\\t\"0.072447859\"\\t\"0\"\\t\"0.22640177\"\\t\"0.120535714\"\\t\"1\"\\t\"1\"\\t\"55\"\\t\"0\"\\t\"2240\"\\t\"258\"\\t\"11\"\\t\"0.166666667\"\\t\"0.057613169\"\\t\"1\"',\n",
       " '\"http://www.dumblittleman.com/2007/12/10-foolproof-tips-for-better-sleep.html\"\\t\"6684\"\\t\"{\"\"title\"\":\"\"10 Foolproof Tips for Better Sleep \"\",\"\"body\"\":\"\"There was a period in my life when I had a lot of problems with sleep It took me very long to fall asleep I was easily awaken and I simply wasn t getting enough of rest at night I didn t want to take medication and this led me to learn several tips and tricks that really helped me to overcome my insomnia Some of these tips I try to follow regularly Don t worry about not getting enough sleep Try not to worry about how much you sleep Such worrying can start a cycle of negative thoughts that contribute to a condition known as learned insomnia Learned insomnia occurs when you worry so much about whether or not you will be able to get adequate sleep that the bedtime rituals and behavior actually trigger insomnia Don t force yourself to sleep The very attempt of trying to do so actually awakes you making it more difficult to sleep Go to bed only when you are feeling really tired and sleepy Don t look at the alarm clock at night Looking at the clock promotes increased anxiety and obsession about time Body heating procedures Some studies suggest that soaking in hot water before going to bed can ease the transition into a deeper sleep Avoid oversleep Don t oversleep to make up for a poor night s sleep Doing so for even a couple of days can reset your body clock and make it harder for you to sleep at night Sex Sex is a well known nighttime stress reliever Healthy sex life enhances your relationship relaxes your body releases happy chemicals and even promotes wellness And it welcomes sleep Avoid alcohol as a sleeping aid Avoid the use of alcohol in the late evening The most common myth found among people is that they believe alcohol helps in the sleep But the fact is alcohol may initially act as sedative but it produces a number of sleep impairing effects in the long run Associate your bed and bedroom with sleep and sex only Don t watch TV eat or read in bed Although these things help some people sleep they can also give your brain the idea that bed isn t just for sleeping and this can keep you awake Naps If you suffer from insomnia try not taking a nap If the goal is to sleep more during the night napping may steal hours desired later on If you re a regular napper and experiencing difficulty falling or staying asleep at night give up the nap and see what happens Written by C Simmons of HealthAssist net dumb little man shares ideas to make the everyday person more productive in life. expect to read tips on finance, saving money, business, and some diy for the house. tips,diy,money,finance,advice,productivity,efficient,technology,saving,software,business,tools\"\",\"\"url\"\":\"\"dumblittleman 2007 12 10 foolproof tips for better sleep html\"\"}\"\\t\"health\"\\t\"0.801248\"\\t\"1.543103448\"\\t\"0.4\"\\t\"0.1\"\\t\"0.016666667\"\\t\"0\"\\t\"0.480724749\"\\t\"0\"\\t\"0\"\\t\"0.095860566\"\\t\"0\"\\t\"0.265655744\"\\t\"0.035343035\"\\t\"1\"\\t\"0\"\\t\"24\"\\t\"0\"\\t\"2737\"\\t\"120\"\\t\"5\"\\t\"0.041666667\"\\t\"0.100858369\"\\t\"1\"',\n",
       " '\"http://bleacherreport.com/articles/1205138-the-50-coolest-jerseys-you-didnt-know-existed?show_full=\"\\t\"9006\"\\t\"{\"\"title\"\":\"\"The 50 Coolest Jerseys You Didn t Know Existed coolest jerseys you haven\\'t seen\"\",\"\"body\"\":\"\"Jersey sales is a curious business Whether you re buying the stylish top to represent your favorite team player or color you re always missing out on better artwork With No 18 Colts jerseys continuing to flood the streets it s about time we educate the sports public about the real masterpieces that have yet to be embraced Forget importance or legacy these upcoming selections will be based solely on visual brilliance We ve scoured the surface of the earth in hopes of finding the ultimate collection of sports jerseys And it s about time we share the findings Here are 50 ridiculous jerseys to brighten your day Grab a camera these shots don t come along often 50 Philadelphia Phillies Throwback Jersey With a curvaceous P on the front and a kelly green backdrop it s time to blow the mothballs off of this legendary masterpiece Solid hard nosed and pure Meet Pete Rose 49 Houston Aeros Away Jersey These designers have thrust their Aeros into the upper tier of the American Hockey League with this curious creation Aggressive yet elegant 48 Florida Panthers 3rd Jersey Blue and darker blue aren t exactly Panthers colors but this jersey as a whole reeks of solid domination On the other hand it also screams Pittsburgh Penguins 47 Pittsburgh Pirates Negro League Jersey There s nothing like Heritage Week to bring back the old days The Homestead Grays clearly believed in simplicity and class Less continues to be more 46 Edmonton Oil Kings Military Jersey This Western Hockey League club is prepared to do whatever it takes to cement its name in history forever And that includes fashioning a Jackson Pollack esque display of paint madness More simply referred to as the Military Appreciation Jerseys Abstract art at its finest 45 Cincinnati Stingers Away Jersey This 70s World Hockey Association masterpiece can t possibly be gulped all at once It s too powerful Here is the rest 44 Kingston Frontenacs Don Cherry Jersey The Kingston Frontenacs are still several suits away from reaching greatness but the Ontario Hockey League just got a lot cooler with this mockery of NHL analyst and nut Don Cherry Risky business though as this can often be misconstrued as brutal 43 Atlanta Braves Throwback Jersey Worn by Hank Aaron and his pack of Braves during the franchise s first two seasons in Atlanta 1966 67 this simple off white piece of cloth is a pleasant vacation for the pupils With a player number replacing the chest high tomahawk and an iconic badge on the sleeve these jerseys present solid compliments to the everyday white and red 42 Montreal Canadiens Throwback Jersey When the Canadiens broke out their 1910 11 Centennial jerseys few were ready for such festive aggression An early Christmas present 41 Missouri Tigers New Jersey Forget the heart of a lion This Missouri squad is ready to prove they possess the mind of a Tiger Enhanced durability makes these cats ferocious 40 North Stars Jersey Minnesota enjoyed the green and yellow North Stars from 1967 to 1993 before they moved to Dallas and became the Stars Minnesota Wild 2000 present brass evidently decided not to follow suit 39 Pittsburgh Steelers Throwback Jersey If these 80th season jerseys truly are inspired by bumblebees then due credit goes to those talented honey dwellers They may be tough on the eyes but these obtuse jerseys are a refreshing and necessary addition to the NFL Only in doses though 38 France New Away Jersey The stripe work may seem excessive but let s be real You can never have enough stripes Just as long as France doesn t get mistaken for a pack of referees 37 Anaheim Ducks Goldberg Jersey David Wright continues to amaze us with his potent disregard for the routine This is no regular Ducks sweater But it s clearly phenomenal 36 Montreal Canadiens Centennial Jersey These 1912 1913 throwbacks give us another reason to love stripes But we re still searching for Waldo 35 Turkey New Away Jersey Simple yet effective is the direction these designers seemed headed for The edgy lettering and gnarly ray of light shining from the back makes this potent presentation almost magical 34 Halifax Mooseheads Jersey The firm red shoulders pads curved lettering and complimentary script are the main ingredients for artistic brilliance The CHL is simply epic 33 California Golden Seals Jersey One of six teams added to the league as part of the 1967 NHL expansion the California Golden Seals never truly grasped the importance of winning on the ice they moved to Cleveland and became the Barons in 1976 They were clearly more focused on keeping their elegant white skates squeaky clean 32 Tampa Bay Lightning 3rd Jersey Released in 08 these silver trimmed knockouts are subtly intense It s refreshing The diagonal style lettering is a slight but necessary detail 31 Miami Heat Orange and Pink Jersey They only played in the ABA for two years 1968 1970 but that didn t keep today s superstars from remembering the Miami Floridians White pink orange black It might be too festive Though never too unique 30 Mighty Ducks of Anaheim 3rd Jersey Not only does the centerpiece bird look like a steroid struck or murderous version of Daffy Duck but he also appears ready to dominate the ice They re far from traditional and far from boring 29 Connecticut Whale Jersey Since 2010 this AHL uniform has dominated the competition The Hartford Whalers esque feel gives the Whale a mighty advantage on camera 28 Los Angeles Kings Throwback Jersey This purple and yellow elegance has rarely let down the illustrious Lakers so perhaps the Kings should turn back the clock and embrace their retro look Beating the Devils for the massive Stanley Cup would be a memorable first step to a new tradition 27 Maryland Terrapins New Jersey If the checkered oddly shaped new Maryland jerseys don t get your senses riled up perhaps their gold alternates will Loving the shoulder pad creativity at this moment although this wild flow may become a visual issue in the future 26 Portugal New Away Jersey The precise coloring cornered numbering and vibrant crosshairs make Portugal s new away kit one for the ages Quality over quantity 25 ZSC Lions Victoria Cup Jersey Founded in 1930 the ZSC Lions are clearly experienced in the art of victory But this Switzerland club proved they were more than just a stylish crew when they beat the Chicago Blackhawks in 2009 for the Victoria Cup trophy 24 Boston Bruins 3rd Jersey With a bear hogging the center there s little about this jersey that doesn t scream amazing We may despise Boston s hockey club but we love their Bruin 23 Washington Redskins Alternate Jersey These yellow and red studs are a refreshing clean cut addition to the often cliched gridiron fashion business Letter circling on the helmet makes this a knockout 22 Rutgers Scarlet Knights New Jersey Underneath this fresh style is an exuberant smile unable to contain itself Oh yes look closely The Scarlet Knights seem ready for battle 21 Montreal Canadiens Centennial Jersey Word has it that this blue masterpiece was only worn for one year 1909 1910 but man what a year it was Time for a comeback 20 England New Away Jersey Rugby is already grueling enough but with a duel kit like this few others have a chance The staredown is classic 19 New York Rangers Heritage Jersey 18 Memphis Grizzlies Throwback Jersey As our eyes steadily adjust to the vibrant presentation it becomes clear that the Memphis Grizzlies have an intricate plan for victory Pleasantly distract all viewers 17 France New Home Jersey A ferocious blue display is perfectly complimented by slight gold lettering on the chest and sleeves Nike s new France home kit has all of Western Europe in a euphoric state 16 San Antonio Spurs Throwback Jersey From 1967 to 1973 the Dallas Chaparrals were easily the most stunning pack of ballers in the ABA And then they moved to San Antonio long before the Spurs decided to pay necessary tribute 15 New York Mets Throwback Jersey It seems the Mets have finally realized the atrocity surrounding their black uniforms A legendary NY scripted on their chest seals the deal Respect for baseball s former New York Giants 14 Quidditch Olympics Jersey The nerdiest sport on the planet just became perhaps the most fashionable as well with the introduction of their 2012 Summer Olympic tops Muggles may not possess the power to soar on broom sticks but they clearly know what it takes to look good 13 Oakland Athletics Throwback Jersey Just picture mustache extraordinaire Rollie Fingers sporting the green and yellow stirrups Truly kicking it old school 12 Chicago Blackhawks Throwback Jersey This 1936 inspired sweater does everything to escape the norm The tribal logo sets the tone while the cream white and red striping puts the finishing touches Epic 11 Texas Rangers Throwback Jersey This 1976 powder blue home uniform rests officially as one of the best to have ever graced the diamond The colorfully striped rims are a nice touch 10 Detroit Red Wings Throwback Jersey During their Cougar days Detroit s beloved franchise fashioned an intricate yet clean arsenal This eye pleasing presentation would define the Original Six and the NHL for years to come 9 Croatia New Home Jersey The design team designated to Croatia s EURO 2012 home kit seems to be simultaneously poking at greatness and insanity But let s be real this is pure genius 8 Ottawa Senators Heritage Jersey It must ve been tough for Senators captain Daniel Alfredsson to contain his exuberance during this moment and we respect him for trying Sure the design is creatively perfect but it s the story behind the cloth that has us in a frenzy All it took was one suggestion from graphic designer Jacob Barrette on an Ottawa Senators message board 7 Chicago White Sox Throwback Jersey Dick Allen s hipster glasses and epic stare hog this intense moment but we re still intrigued by the powder blue button down The Ambrose Burnside sideburns complete this masterful shot 6 Los Angeles Lakers Throwback Jersey The Minneapolis Lakers would only last 13 years 1947 1960 but they certainly cemented their spot in sports lore with five George Mikan led championships and vibrant suits Perfection 5 Great Britain New Olympics Jersey Thank you Leicester Riders forward and national team captain Andrew Sullivan for the pink touch The glistening top reeks of class and the solid neck of aggression Nicely done 4 Cleveland Cavaliers Hardwood Classic Jersey We saw the Cavs sport a blue and orange top during LeBron James days but not so much this retro piece The 1970 s wine and gold look is a keeper 3 Ohio State Buckeyes Throwback Jersey In honor of the 1954 Buckeyes team the Ohio State football team was seen repping these artistic keepers at Michigan Stadium several years back From the intricate sleeve work to the simple yet engaging helmets these uniforms epically compliment the gridiron 2 Golden State Warriors Throwback Jersey A faint blue and red trimming cements this less is more masterpiece in hardwood lore for good Purely classic 1 Brooklyn Dodgers Throwback Jersey What a brilliant shot for all Dodgers fans only in California of course The 1940 s road alternates help these West Coast superstars look the part rankings\\\\/list, multiple sports, b\\\\/r swagger, partners, sfgate, latimes, cool lists, outbrain, boston partners jersey sales is a curious business. whether you\\'re buying the stylish top to represent your favorite team, player or color, you\\'re always missing out on better artwork.  with no... jersey sales is a curious business. whether you\\'re buying the stylish top to represent your favorite team, player or color, you\\'re always missing out on better artwork.  with no...\"\",\"\"url\"\":\"\"bleacherreport articles 1205138 the 50 coolest jerseys you didnt know existed show_full\"\"}\"\\t\"sports\"\\t\"0.719157\"\\t\"2.676470588\"\\t\"0.5\"\\t\"0.222222222\"\\t\"0.12345679\"\\t\"0.043209877\"\\t\"0.446143274\"\\t\"0\"\\t\"0\"\\t\"0.024908425\"\\t\"0\"\\t\"0.228887247\"\\t\"0.050473186\"\\t\"1\"\\t\"1\"\\t\"14\"\\t\"0\"\\t\"12032\"\\t\"162\"\\t\"10\"\\t\"0.098765432\"\\t\"0.082568807\"\\t\"0\"',\n",
       " '\"http://www.conveniencemedical.com/genital-herpes-home.php\"\\t\"7018\"\\t\"{\"\"url\"\":\"\"conveniencemedical genital herpes home php\"\",\"\"title\"\":\"\"Genital Herpes Treatment \"\",\"\"body\"\":\"\"Genital herpes is caused by herpes simplex virus HSV 1 the most common type usually appears as cold sores on the mouth and lips whereas HSV 2 affects the genitals buttocks and rectum Patient s have reported that symptoms can appear five days after coming into contact with the virus on the other hand it may be months or even years until the first outbreak Unfortunately once you have caught the genital herpes virus there is no cure for it You can prevent the virus spreading and manage pain caused by the blisters with a course of anti viral treatment Should I have sex if I have genital herpes When you have symptoms of genital herpes HSV is very contagious when blisters are present and there is a high chance of passing on the virus if you have sex You should not have sex from the time symptoms first start until they are fully over If you do have sex using a condom may not fully protect against passing on the virus as the condom only protects the area that is covered When you do not have symptoms which is most of the time It is less likely that you will pass the virus on when you have sex However some virus will be present on the genital skin surface from time to time although infrequently So there is still a small chance that you may pass on the virus when you have sex when you do not have symptoms People who take antiviral medication long term to prevent recurrences have a reduced risk of passing on the virus What is aciclovir it is an antiviral tablet which aims to reduce the severity and duration of an outbreak of genital herpes How do I take the medicine One tablet three times a day for five days Start taking it as soon as you feel a herpes outbreak beginning What are the potential side effects Like any medication these include nausea vomiting diarrhoea or an allergic rash can occur Should this occur you should stop this medication immediately and contact us via your secure online patient record How effective is this treatment Should be effective in most patients if taken as soon as the signs of an outbreak occur How long will one course of treatment last One course is sufficient for a single outbreak genital herpes is a sexually transmitted infection caused by the herpes simplex virus (hsv), and is transmitted through unprotected and oral sex. anyone who is sexually active can catch genital herpes and if the genital herpes is left untreated it may spread to other parts of the body.\"\"}\"\\t\"?\"\\t\"?\"\\t\"119\"\\t\"0.745454545\"\\t\"0.581818182\"\\t\"0.290909091\"\\t\"0.018181818\"\\t\"0.434639175\"\\t\"0\"\\t\"0\"\\t\"0.01984127\"\\t\"0\"\\t\"0.298299595\"\\t\"0.038636364\"\\t\"?\"\\t\"0\"\\t\"12\"\\t\"?\"\\t\"4368\"\\t\"55\"\\t\"3\"\\t\"0.054545455\"\\t\"0.087356322\"\\t\"0\"',\n",
       " '\"http://gofashionlane.blogspot.tw/2012/06/american-wild-child.html\"\\t\"8685\"\\t\"{\"\"title\"\":\"\"fashion lane American Wild Child \"\",\"\"body\"\":\"\"Our favorite summer holiday is just around the corner and boy do we have a heavy dose of outfit inspiration just waiting for you American Wild Child the newest breed of trend reports features what we know are stellar 4th of July looks This summer holiday is the perfect occasion to rock all things Americana influenced whether it be a straight up American Flag over sized scarf styling note If you are beach pool bound this beauty makes for an amazing wrap cover up Winning a Furst of a Kind American flag Denim vest a red white and blue tie dye tank or even our favorite shoe freaks the Damsel bootie in the American flag print No matter what you are doing you are sure going to find a rad outfit that screams I am proud to be an American All pieces in the Trend Report are available at an LF near you PH Daniel Kincaid Styling Alexandra Sherman MU Hair Kait Marie Model Beth Whitson Vision \"\",\"\"url\"\":\"\"gofashionlane blogspot tw 2012 06 american wild child html\"\"}\"\\t\"arts_entertainment\"\\t\"0.22111\"\\t\"0.773809524\"\\t\"0.215053763\"\\t\"0.053763441\"\\t\"0.043010753\"\\t\"0.043010753\"\\t\"0.579596413\"\\t\"0\"\\t\"0\"\\t\"0.039568345\"\\t\"0\"\\t\"0.218978009\"\\t\"0.311377246\"\\t\"1\"\\t\"0\"\\t\"21\"\\t\"0\"\\t\"1287\"\\t\"93\"\\t\"3\"\\t\"0.548387097\"\\t\"0.064327485\"\\t\"1\"',\n",
       " '\"http://www.insidershealth.com/article/racing_for_recovery/3471\"\\t\"3402\"\\t\"{\"\"url\"\":\"\"insidershealth article racing for recovery 3471\"\",\"\"title\"\":\"\"Racing For Recovery by Dean Johnson racing for recovery by dean johnson - - insidershealth.com\"\",\"\"body\"\":\"\"Racing For Recovery is the growing idea that drug addiction and alcoholism can be cured by focusing one s energies towards something positive Eddie Freas tell CNN that I feel better when I m working out It does wonders for the mind The reason I started running it was a switch that went off in my head I started feeling positive and feeling great about myself It is true that dopamine levels can be increased by strenuous physical activity but is simply getting your fix in a healthy way actually healthy for you in the long run Freas admits to trying Alcoholics Anonymous and rehab but says the institutions did nothing to cure his drug addiction perhaps that is because of AA s non belief in the ability of a true addict to get cured they look at it as a disease that like cancer can only go into remission In any case Freas is sober today and has gotten there by doing something he loves that much is commendable Whether or not this strategy will better his quality of life or keep him sober over time has yet to be seen The trick apparently isn t just to exercise but to find something anything that will help This Content is restricted to our Community Login or Sign Up for an account to continue reading an insidershealth.com article regarding - racing for recovery by dean johnson. racing for recovery is the growing idea that drug addiction and alcoholism can be cured by focusing one&rsquo;s energies towards something positive. , , alternative health, article, homeopathic, health, natural medicine, natural cures, community\"\"}\"\\t\"?\"\\t\"?\"\\t\"1.883333333\"\\t\"0.71969697\"\\t\"0.265151515\"\\t\"0.113636364\"\\t\"0.015151515\"\\t\"0.49934811\"\\t\"0\"\\t\"0\"\\t\"0.02661597\"\\t\"0\"\\t\"0.173745927\"\\t\"0.025830258\"\\t\"?\"\\t\"0\"\\t\"5\"\\t\"?\"\\t\"27656\"\\t\"132\"\\t\"4\"\\t\"0.068181818\"\\t\"0.148550725\"\\t\"0\"',\n",
       " '\"http://www.valetmag.com/the-handbook/features/31-days/index.php?index1_middle_feature\"\\t\"477\"\\t\"{\"\"title\"\":\"\"Valet The Handbook 31 Days 31 days\"\",\"\"body\"\":\"\"Resolutions are for suckers Instead of swearing off sugar for all of two weeks simply vow to live better For the fourth annual 31 Days we re offering you all the tips and tricks needed to shape up for 2012 Each day brings new expert advice on how to look your best and live life to the fullest resolutions are for suckers. instead of swearing off sugar (for all of two weeks), simply vow to live better. for the fourth annual 31 days, we\\'re offering you all the tips and tricks needed to shape up for 2012. each day brings new, expert advice on how to look your best, live life to the fullest and have a little fun while doing it. here\\'s to starting the new decade off in style. valet, valetmag, 31 days, how to tips and tricks, style how tos resolutions are for suckers. instead of swearing off sugar (for all of two weeks), simply vow to live better. we show you how.\"\",\"\"url\"\":\"\"valetmag the handbook features 31 days index php index1 middle feature\"\"}\"\\t\"?\"\\t\"?\"\\t\"0.471502591\"\\t\"0.190721649\"\\t\"0.036082474\"\\t\"0\"\\t\"0\"\\t\"0.383199079\"\\t\"0\"\\t\"0\"\\t\"0.021705426\"\\t\"0\"\\t\"0.11496229\"\\t\"1.136645963\"\\t\"1\"\\t\"0\"\\t\"17\"\\t\"0\"\\t\"2471\"\\t\"194\"\\t\"7\"\\t\"0.644329897\"\\t\"0.125\"\\t\"1\"',\n",
       " '\"http://www.howsweeteats.com/2010/03/24/cookies-and-cream-brownies/\"\\t\"6731\"\\t\"{\"\"url\"\":\"\"howsweeteats 2010 03 24 cookies and cream brownies\"\",\"\"title\"\":\"\"Cookies and Cream Brownies How Sweet It Is \"\",\"\"body\"\":\"\"More brownies It seems that I can t get through one full week without trying a new brownie combination Just wait until you see the next one I am going to experiment with These are super simple I used my go to brownie recipe and just added a few Oreos You could even do the same with boxed brownies Just make the frosting and add some more cookies I only frosted a few because I just don t love frosting on brownies Mr How Sweet really enjoyed the frosted ones If you waste a good 3 4 of your day on Twitter like I do you will know that last night Mr How Sweet brought me home a dozen hot pink roses I didn t make him dinner my half unpacked mess is strewn in 4 different rooms and I was in holey sweat pants once he got home Oh yes and I was half asleep while he professed his undying love to me And I just continued to lay there Sounds eerily similar to the night he proposed Good thing there were brownies on the counter As my dad says marriage is rarely 50 50 The cookies somewhat melted into the batter once baked but if you look closely you can see some cookies inside the brownies That is the best part The frosting is the same frosting I used for the cookies and cream cupcakes It is creamy delicious and looks like ice cream Can t beat that I think out of all the brownies I have made in the past few months these ones may be the best I love the texture Chewy gooey brownies crunchy cookies Cookies and Cream Brownies 2 3 squares baking chocolate 1 2 lb butter 2 cups sugar 4 eggs 1 cup flour 1 teaspoon vanilla 8 Oreo cookies crumbled Preheat oven to 350 Melt butter and baking sqaures in a microwave safe bowl In the mixing bowl cream melted butter and chocolate mixture Add in sugar and vanilla and beat until fluffy about 2 3 minutes Add in eggs beating one at a time Add flour and stir just until combined Fold in cookie crumbles Pour into greased 9 x 13 baking dish I used a 8 x 8 square cake pan Bake for 35 45 minutes or until just barely done Cool before frosting I changed this recipe slightly from my other brownie posts so if you have recently tried it try adding 2 more eggs Cookies and Cream Frosting 2 sticks butter softened 1 1 2 2 lbs powdered sugar 1 teaspoon vanilla 1 2 tablespoons milk 10 Oreo cookies Mix butter and powdered sugar together adding sugar gradually to the mix Add vanilla Mix in milk adding more if needed Continue to add milk and sugar until it reaches the desired consistency Crush Oreos and fold them into the frosting I don t think these brownies will last another evening in our house Ha who am I kidding They may not even last until Mr How Sweet gets home Now I need to find some spectacular recipe to make for dinner tonight And remind Mr How Sweet that he married someone 10 years younger not an old granny Tags brownies chocolate recipes more brownies! it seems that i can\\'t get through one full week without trying a new brownie combination. just wait until you see the next one i am going to brownies,chocolate,recipes\"\"}\"\\t\"?\"\\t\"?\"\\t\"2.41011236\"\\t\"0.469325153\"\\t\"0.101226994\"\\t\"0.018404908\"\\t\"0.003067485\"\\t\"0.465859401\"\\t\"0\"\\t\"0\"\\t\"0.012\"\\t\"0\"\\t\"0.205116573\"\\t\"0.20626151\"\\t\"?\"\\t\"1\"\\t\"14\"\\t\"?\"\\t\"11459\"\\t\"326\"\\t\"4\"\\t\"0.236196319\"\\t\"0.094412331\"\\t\"1\"']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_header.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = train_no_header.map(lambda x:x.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"',\n",
       " '\"4042\"',\n",
       " '\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ball The predictions are part of an annual tradition for the Armonk New York based company which surveys its 3 000 researchers to find five ideas expected to take root in the next five years IBM the world s largest provider of computer services looks to Silicon Valley for input gleaning many ideas from its Almaden research center in San Jose California Holographic conversations projected from mobile phones lead this year s list The predictions also include air breathing batteries computer programs that can tell when and where traffic jams will take place environmental information generated by sensors in cars and phones and cities powered by the heat thrown off by computer servers These are all stretch goals and that s good said Paul Saffo managing director of foresight at the investment advisory firm Discern in San Francisco In an era when pessimism is the new black a little dose of technological optimism is not a bad thing For IBM it s not just idle speculation The company is one of the few big corporations investing in long range research projects and it counts on innovation to fuel growth Saffo said Not all of its predictions pan out though IBM was overly optimistic about the spread of speech technology for instance When the ideas do lead to products they can have broad implications for society as well as IBM s bottom line he said Research Spending They have continued to do research when all the other grand research organizations are gone said Saffo who is also a consulting associate professor at Stanford University IBM invested 5 8 billion in research and development last year 6 1 percent of revenue While that s down from about 10 percent in the early 1990s the company spends a bigger share on research than its computing rivals Hewlett Packard Co the top maker of personal computers spent 2 4 percent last year At Almaden scientists work on projects that don t always fit in with IBM s computer business The lab s research includes efforts to develop an electric car battery that runs 500 miles on one charge a filtration system for desalination and a program that shows changes in geographic data IBM rose 9 cents to 146 04 at 11 02 a m in New York Stock Exchange composite trading The stock had gained 11 percent this year before today Citizen Science The list is meant to give a window into the company s innovation engine said Josephine Cheng a vice president at IBM s Almaden lab All this demonstrates a real culture of innovation at IBM and willingness to devote itself to solving some of the world s biggest problems she said Many of the predictions are based on projects that IBM has in the works One of this year s ideas that sensors in cars wallets and personal devices will give scientists better data about the environment is an expansion of the company s citizen science initiative Earlier this year IBM teamed up with the California State Water Resources Control Board and the City of San Jose Environmental Services to help gather information about waterways Researchers from Almaden created an application that lets smartphone users snap photos of streams and creeks and report back on conditions The hope is that these casual observations will help local and state officials who don t have the resources to do the work themselves Traffic Predictors IBM also sees data helping shorten commutes in the next five years Computer programs will use algorithms and real time traffic information to predict which roads will have backups and how to avoid getting stuck Batteries may last 10 times longer in 2015 than today IBM says Rather than using the current lithium ion technology new models could rely on energy dense metals that only need to interact with the air to recharge Some electronic devices might ditch batteries altogether and use something similar to kinetic wristwatches which only need to be shaken to generate a charge The final prediction involves recycling the heat generated by computers and data centers Almost half of the power used by data centers is currently spent keeping the computers cool IBM scientists say it would be better to harness that heat to warm houses and offices In IBM s first list of predictions compiled at the end of 2006 researchers said instantaneous speech translation would become the norm That hasn t happened yet While some programs can quickly translate electronic documents and instant messages and other apps can perform limited speech translation there s nothing widely available that acts like the universal translator in Star Trek Second Life The company also predicted that online immersive environments such as Second Life would become more widespread While immersive video games are as popular as ever Second Life s growth has slowed Internet users are flocking instead to the more 2 D environments of Facebook Inc and Twitter Inc Meanwhile a 2007 prediction that mobile phones will act as a wallet ticket broker concierge bank and shopping assistant is coming true thanks to the explosion of smartphone applications Consumers can pay bills through their banking apps buy movie tickets and get instant feedback on potential purchases all with a few taps on their phones The nice thing about the list is that it provokes thought Saffo said If everything came true they wouldn t be doing their job To contact the reporter on this story Ryan Flinn in San Francisco at rflinn bloomberg net To contact the editor responsible for this story Tom Giles at tgiles5 bloomberg net by 2015, your mobile phone will project a 3-d image of anyone who calls and your laptop will be powered by kinetic energy. at least that\\\\u2019s what international business machines corp. sees in its crystal ball.\"\",\"\"url\"\":\"\"bloomberg news 2010 12 23 ibm predicts holographic calls air breathing batteries by 2015 html\"\"}\"',\n",
       " '\"business\"',\n",
       " '\"0.789131\"',\n",
       " '\"2.055555556\"',\n",
       " '\"0.676470588\"',\n",
       " '\"0.205882353\"',\n",
       " '\"0.047058824\"',\n",
       " '\"0.023529412\"',\n",
       " '\"0.443783175\"',\n",
       " '\"0\"',\n",
       " '\"0\"',\n",
       " '\"0.09077381\"',\n",
       " '\"0\"',\n",
       " '\"0.245831182\"',\n",
       " '\"0.003883495\"',\n",
       " '\"1\"',\n",
       " '\"1\"',\n",
       " '\"24\"',\n",
       " '\"0\"',\n",
       " '\"5424\"',\n",
       " '\"170\"',\n",
       " '\"8\"',\n",
       " '\"0.152941176\"',\n",
       " '\"0.079129575\"',\n",
       " '\"0\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = records.map(\n",
    "    lambda a_list:[x.replace('\\\"', '') for x in a_list]\n",
    "    ).map(\n",
    "    lambda x:x[4:]\n",
    "    ).map(\n",
    "    lambda a_list:([(0.0 if i=='?' else float(i)) for i in a_list[:-1] ] +[int(a_list[-1]), ])\n",
    "    ).map(\n",
    "    lambda x:LabeledPoint(x[-1], x[:-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdata = records.map(\n",
    "    lambda a_list:[x.replace('\\\"', '') for x in a_list]\n",
    "    ).map(\n",
    "    lambda x:x[4:]\n",
    "    ).map(\n",
    "    lambda a_list:([(0.0 if i=='?' else float(i)) for i in a_list[:-1] ] +[int(a_list[-1]), ])\n",
    "    ).map(\n",
    "    lambda a_list:([(0.0 if i<0.0 else i) for i in a_list[:-1] ] +[a_list[-1], ])\n",
    "    ).map(\n",
    "    lambda x:LabeledPoint(x[-1], x[:-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029]),\n",
       " LabeledPoint(1.0, [0.996526,2.382882883,0.562015504,0.321705426,0.120155039,0.042635659,0.525448029,0.0,0.0,0.072447859,0.0,0.22640177,0.120535714,1.0,1.0,55.0,0.0,2240.0,258.0,11.0,0.166666667,0.057613169]),\n",
       " LabeledPoint(1.0, [0.801248,1.543103448,0.4,0.1,0.016666667,0.0,0.480724749,0.0,0.0,0.095860566,0.0,0.265655744,0.035343035,1.0,0.0,24.0,0.0,2737.0,120.0,5.0,0.041666667,0.100858369]),\n",
       " LabeledPoint(0.0, [0.719157,2.676470588,0.5,0.222222222,0.12345679,0.043209877,0.446143274,0.0,0.0,0.024908425,0.0,0.228887247,0.050473186,1.0,1.0,14.0,0.0,12032.0,162.0,10.0,0.098765432,0.082568807]),\n",
       " LabeledPoint(0.0, [0.0,119.0,0.745454545,0.581818182,0.290909091,0.018181818,0.434639175,0.0,0.0,0.01984127,0.0,0.298299595,0.038636364,0.0,0.0,12.0,0.0,4368.0,55.0,3.0,0.054545455,0.087356322]),\n",
       " LabeledPoint(1.0, [0.22111,0.773809524,0.215053763,0.053763441,0.043010753,0.043010753,0.579596413,0.0,0.0,0.039568345,0.0,0.218978009,0.311377246,1.0,0.0,21.0,0.0,1287.0,93.0,3.0,0.548387097,0.064327485]),\n",
       " LabeledPoint(0.0, [0.0,1.883333333,0.71969697,0.265151515,0.113636364,0.015151515,0.49934811,0.0,0.0,0.02661597,0.0,0.173745927,0.025830258,0.0,0.0,5.0,0.0,27656.0,132.0,4.0,0.068181818,0.148550725]),\n",
       " LabeledPoint(1.0, [0.0,0.471502591,0.190721649,0.036082474,0.0,0.0,0.383199079,0.0,0.0,0.021705426,0.0,0.11496229,1.136645963,1.0,0.0,17.0,0.0,2471.0,194.0,7.0,0.644329897,0.125]),\n",
       " LabeledPoint(1.0, [0.0,2.41011236,0.469325153,0.101226994,0.018404908,0.003067485,0.465859401,0.0,0.0,0.012,0.0,0.205116573,0.20626151,0.0,1.0,14.0,0.0,11459.0,326.0,4.0,0.236196319,0.094412331]),\n",
       " LabeledPoint(0.0, [0.816604,2.506527415,0.637755102,0.293367347,0.091836735,0.048469388,0.592321755,0.0,0.0,0.056497175,0.0,0.223003543,0.511363636,1.0,1.0,53.0,0.0,4401.0,392.0,0.0,0.160714286,0.073684211]),\n",
       " LabeledPoint(0.0, [0.89156,4.986111111,0.64,0.426666667,0.32,0.293333333,0.521064302,0.004065041,0.0,0.162689805,0.0,0.24691196,0.06097561,0.0,1.0,40.0,0.0,2701.0,75.0,8.0,0.186666667,0.115384615]),\n",
       " LabeledPoint(1.0, [0.872323,3.056910569,0.595588235,0.227941176,0.044117647,0.014705882,0.573109244,0.0,0.0,0.074576271,0.0,0.236280941,0.08411215,1.0,1.0,64.0,0.0,1062.0,136.0,9.0,0.169117647,0.180327869]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,0.0,0.0,21.0,-1.0,0.0,0.057692308,0.0,0.328343661,-1.0,0.0,0.0,0.0,0.0,4091.0,5.0,11.0,0.0,0.083333333]),\n",
       " LabeledPoint(0.0, [0.559991,2.299492386,0.547413793,0.206896552,0.056034483,0.017241379,0.473964868,0.0,0.0,0.078431373,1.0,0.205378622,0.036423841,0.0,1.0,37.0,0.0,3610.0,232.0,11.0,0.215517241,0.080204778]),\n",
       " LabeledPoint(0.0, [0.548963,0.990430622,0.522522523,0.108108108,0.009009009,0.0,0.414154653,0.0,0.0,0.054992764,0.0,0.189932886,0.075630252,0.0,0.0,54.0,0.0,883.0,222.0,4.0,0.013513514,0.059602649]),\n",
       " LabeledPoint(0.0, [0.598149,0.929824561,0.068965517,0.0,0.0,0.0,0.478354978,0.0,0.0,0.157575758,0.0,0.310008857,0.276315789,1.0,0.0,50.0,0.0,268.0,58.0,2.0,0.137931034,0.107142857]),\n",
       " LabeledPoint(0.0, [0.77292,2.083333333,0.421052632,0.178947368,0.042105263,0.0,0.462994836,0.0,0.0,0.099778271,0.0,0.232048045,0.275862069,1.0,0.0,49.0,0.0,852.0,95.0,8.0,0.305263158,0.139737991]),\n",
       " LabeledPoint(1.0, [0.287884,3.164383562,0.586206897,0.275862069,0.172413793,0.149425287,0.520414776,0.0,0.0,0.035864979,0.0,0.265881624,0.091549296,1.0,1.0,23.0,0.0,3509.0,87.0,2.0,0.402298851,0.09057971]),\n",
       " LabeledPoint(1.0, [0.585389,2.826771654,0.669064748,0.237410072,0.064748201,0.021582734,0.56279509,0.0,0.0,0.07345576,0.0,0.247035291,0.053254438,1.0,1.0,55.0,0.0,1511.0,139.0,9.0,0.165467626,0.065217391]),\n",
       " LabeledPoint(0.0, [0.941129,4.103626943,0.515555556,0.297777778,0.186666667,0.106666667,0.412429379,0.0,0.0,0.064489112,0.0,0.212927811,0.693181818,1.0,1.0,53.0,0.0,3243.0,225.0,6.0,0.208888889,0.043859649]),\n",
       " LabeledPoint(1.0, [0.437599,2.033333333,0.516129032,0.172043011,0.043010753,0.0,0.909836066,0.0,0.0,0.105577689,0.0,0.229413592,0.863636364,0.0,1.0,31.0,0.0,1927.0,93.0,5.0,0.043010753,0.078947368]),\n",
       " LabeledPoint(0.0, [0.644872,1.333333333,0.0,0.0,0.0,0.0,0.468309859,0.0,0.0,0.041262136,0.0,0.168818229,0.062962963,0.0,0.0,3.0,0.0,1766.0,11.0,2.0,0.090909091,0.084175084]),\n",
       " LabeledPoint(1.0, [0.0,4.327655311,0.978757515,0.895791583,0.669138277,0.422044088,0.248915158,0.000236,0.0,0.002975637,0.0,0.114361006,0.008274232,1.0,1.0,85.0,0.0,20757.0,4990.0,6.0,0.992785571,0.157953592]),\n",
       " LabeledPoint(1.0, [0.0,1.786407767,0.552631579,0.149122807,0.052631579,0.01754386,21.0,-1.0,0.0,0.081314879,0.0,0.254886052,-1.0,1.0,1.0,20.0,0.0,3760.0,114.0,5.0,0.043859649,0.0]),\n",
       " LabeledPoint(0.0, [0.898192,4.8,0.603448276,0.379310345,0.24137931,0.155172414,0.480519481,0.010638298,0.0,0.173210162,0.0,0.259285272,0.085106383,1.0,1.0,45.0,0.0,1802.0,58.0,9.0,0.155172414,0.055]),\n",
       " LabeledPoint(0.0, [0.348995,3.355263158,0.488372093,0.290697674,0.081395349,0.023255814,0.573825503,0.0,0.0,0.007968127,0.0,0.348059033,0.004608295,1.0,1.0,98.0,1.0,21.0,86.0,0.0,0.0,0.128440367]),\n",
       " LabeledPoint(0.0, [0.85,3.321428571,0.505747126,0.298850575,0.183908046,0.137931034,0.421385706,0.0,0.0,0.00496963,0.0,0.256231197,0.004006869,0.0,1.0,2.0,0.0,67406.0,87.0,7.0,0.091954023,0.097297297]),\n",
       " LabeledPoint(1.0, [0.0,3.417910448,0.541176471,0.270588235,0.176470588,0.117647059,0.644171779,0.0,0.0,0.075471698,0.0,0.245831092,0.166666667,1.0,1.0,68.0,0.0,536.0,85.0,6.0,0.035294118,0.159574468]),\n",
       " LabeledPoint(1.0, [0.0,1.154761905,0.504424779,0.427728614,0.02359882,0.0,0.401048493,0.0,0.0,0.032291667,0.0,0.144492064,1.18487395,0.0,0.0,80.0,0.0,490.0,339.0,4.0,0.005899705,0.033112583]),\n",
       " LabeledPoint(1.0, [0.289158,1.985815603,0.356643357,0.13986014,0.055944056,0.027972028,0.571005917,0.0,0.0,0.027027027,0.0,0.228713696,0.230769231,0.0,1.0,11.0,0.0,9935.0,143.0,2.0,0.027972028,0.198412698]),\n",
       " LabeledPoint(1.0, [0.670358,1.571428571,0.451327434,0.168141593,0.10619469,0.053097345,0.4375,0.0,0.0,0.055214724,0.0,0.257944809,0.087824351,1.0,1.0,12.0,0.0,5500.0,113.0,4.0,0.159292035,0.086065574]),\n",
       " LabeledPoint(0.0, [0.0,1.292682927,0.421965318,0.306358382,0.011560694,0.0,0.527264325,0.0,0.0,0.03820598,0.0,0.233130593,0.110275689,1.0,0.0,28.0,0.0,2531.0,173.0,3.0,0.202312139,0.095115681]),\n",
       " LabeledPoint(1.0, [0.899731,2.166666667,0.447513812,0.20718232,0.074585635,0.03038674,0.828025478,0.0,0.0,0.027660854,0.0,0.191917306,3.625,0.0,1.0,35.0,0.0,4809.0,362.0,6.0,0.472375691,0.085106383]),\n",
       " LabeledPoint(0.0, [0.783732,2.544041451,0.583333333,0.324074074,0.12962963,0.037037037,0.514041514,0.0,0.0,0.082568807,0.0,0.204442786,0.270072993,1.0,1.0,44.0,0.0,3225.0,216.0,16.0,0.189814815,0.039772727]),\n",
       " LabeledPoint(0.0, [0.0,1.888888889,0.59375,0.171875,0.0625,0.046875,0.45,0.0,0.0,0.084805654,0.0,0.259424984,0.005976096,1.0,1.0,18.0,1.0,2419.0,64.0,0.0,0.0,0.089249493]),\n",
       " LabeledPoint(1.0, [0.0,2.618902439,0.707317073,0.33604336,0.119241192,0.051490515,0.478082192,0.0,0.0,0.046441948,0.0,0.220510101,0.054187192,0.0,1.0,28.0,0.0,10422.0,369.0,6.0,0.067750678,0.083538084]),\n",
       " LabeledPoint(0.0, [0.519337,1.549450549,0.417910448,0.114427861,0.004975124,0.004975124,0.569044006,0.0,0.0,0.040764331,0.0,0.256809688,0.183206107,1.0,0.0,37.0,0.0,2382.0,201.0,1.0,0.014925373,0.092307692]),\n",
       " LabeledPoint(1.0, [0.0,2.881944444,0.54822335,0.23857868,0.106598985,0.040609137,0.285714286,0.0,0.0,0.05669145,0.0,0.188223998,0.175824176,1.0,1.0,52.0,0.0,1987.0,197.0,7.0,0.076142132,0.0]),\n",
       " LabeledPoint(1.0, [0.975621,2.188311688,0.569060773,0.149171271,0.055248619,0.027624309,0.430342385,0.0,0.0,0.035058431,0.0,0.243810822,0.01849711,0.0,1.0,25.0,0.0,4885.0,181.0,9.0,0.237569061,0.087616822]),\n",
       " LabeledPoint(0.0, [0.849952,9.8,0.292307692,0.092307692,0.015384615,0.0,0.439756882,0.0,0.0,0.058227848,0.0,0.328052016,0.012345679,0.0,0.0,6.0,0.0,7456.0,65.0,1.0,0.046153846,0.089824561]),\n",
       " LabeledPoint(1.0, [0.487492,2.086956522,0.498194946,0.202166065,0.028880866,0.02166065,0.510095238,0.00204918,0.0,0.008849558,0.0,0.233007607,0.100409836,1.0,1.0,22.0,1.0,8366.0,277.0,0.0,0.523465704,0.124726477]),\n",
       " LabeledPoint(0.0, [0.0,1.76969697,0.381818182,0.181818182,0.048484848,0.006060606,0.596837945,0.0,0.0,0.072669826,0.0,0.203740299,0.413533835,1.0,1.0,54.0,0.0,1200.0,165.0,5.0,0.139393939,0.06122449]),\n",
       " LabeledPoint(1.0, [0.802314,1.783333333,0.214285714,0.071428571,0.014285714,0.0,0.445471349,0.0,0.0,0.070116861,1.0,0.258761956,0.314814815,1.0,1.0,5.0,0.0,9309.0,70.0,8.0,0.114285714,0.06185567]),\n",
       " LabeledPoint(1.0, [0.0,1.158208955,0.50591716,0.428994083,0.023668639,0.0,0.423620026,0.0,0.0,0.029319372,0.0,0.137165504,1.112,0.0,0.0,79.0,0.0,494.0,338.0,4.0,0.00591716,0.025]),\n",
       " LabeledPoint(0.0, [0.504924,2.443983402,0.365517241,0.2,0.072413793,0.055172414,0.464774951,0.0,0.0,0.022196262,0.0,0.149940034,0.918918919,1.0,1.0,76.0,0.0,1078.0,290.0,2.0,0.117241379,0.081081081]),\n",
       " LabeledPoint(0.0, [0.0,2.133333333,0.655737705,0.213114754,0.196721311,0.196721311,0.491408935,0.0,0.0,0.078947368,0.0,0.271765078,0.013355593,1.0,1.0,16.0,1.0,3457.0,61.0,0.0,0.049180328,0.094339623]),\n",
       " LabeledPoint(1.0, [0.0,2.328502415,0.427777778,0.205555556,0.061111111,0.019444444,0.989795918,0.0,0.0,0.026812919,0.0,0.187438789,5.25,0.0,0.0,33.0,0.0,4791.0,360.0,5.0,0.466666667,0.131578947]),\n",
       " LabeledPoint(0.0, [0.729559,4.588477366,0.445255474,0.284671533,0.211678832,0.167883212,0.358770718,0.0,0.0,0.041106719,0.0,0.262156988,0.014787431,0.0,1.0,30.0,0.0,12083.0,274.0,14.0,0.116788321,0.111320755]),\n",
       " LabeledPoint(0.0, [0.0,2.85483871,0.428571429,0.103896104,0.038961039,0.0,0.46393147,0.0,0.0,0.094736842,0.0,0.290880017,0.03379224,0.0,1.0,16.0,0.0,4191.0,77.0,6.0,0.324675325,0.13671875]),\n",
       " LabeledPoint(0.0, [0.530241,2.166666667,0.166666667,0.0,0.0,0.0,0.461511547,0.0,0.0,0.125827815,0.0,0.35780892,0.009146341,1.0,0.0,4.0,0.0,4623.0,18.0,8.0,0.0,0.11261731]),\n",
       " LabeledPoint(1.0, [0.604216,1.236947791,0.686131387,0.03649635,0.00729927,0.003649635,0.519774011,0.0,0.0,0.035453597,0.0,0.179364373,0.381443299,1.0,1.0,37.0,0.0,2493.0,274.0,5.0,0.01459854,0.111111111]),\n",
       " LabeledPoint(0.0, [0.895862,3.073684211,0.65625,0.302083333,0.09375,0.041666667,0.56036036,0.0,0.0,0.018083183,0.0,0.22628767,0.202380952,0.0,1.0,44.0,0.0,2131.0,96.0,0.0,0.0,0.021276596]),\n",
       " LabeledPoint(1.0, [0.753521,1.71641791,0.522058824,0.316176471,0.044117647,0.029411765,0.424012158,0.0,0.0,0.057034221,0.0,0.231050263,0.030405405,1.0,0.0,21.0,0.0,3920.0,136.0,4.0,0.051470588,0.056737589]),\n",
       " LabeledPoint(1.0, [0.548939,2.5,0.684210526,0.255639098,0.037593985,0.015037594,0.466779279,0.0,0.0,0.07311828,0.0,0.267735644,0.011725293,1.0,1.0,34.0,0.0,2897.0,133.0,6.0,0.804511278,0.080536913]),\n",
       " LabeledPoint(0.0, [0.0,2.278481013,0.552419355,0.266129032,0.052419355,0.02016129,21.0,-1.0,0.0,0.044101433,0.0,0.265426121,-1.0,1.0,1.0,13.0,0.0,17131.0,248.0,13.0,0.366935484,0.235294118]),\n",
       " LabeledPoint(1.0, [0.34619,3.660606061,0.591397849,0.370967742,0.279569892,0.225806452,0.48865523,0.0,0.0,0.03030303,0.0,0.274790836,0.038580247,1.0,1.0,24.0,0.0,8191.0,186.0,6.0,0.064516129,0.082148499]),\n",
       " LabeledPoint(0.0, [0.826992,0.222929936,0.103792415,0.063872255,0.003992016,0.0,0.473928158,0.0,0.0,0.009281876,0.0,0.173427509,0.008684864,0.0,0.0,2.0,0.0,28276.0,501.0,6.0,0.003992016,0.082914573]),\n",
       " LabeledPoint(0.0, [0.751641,4.732793522,0.580152672,0.41221374,0.297709924,0.229007634,0.845588235,0.055555556,0.0,0.040860215,0.0,0.21120799,1.111111111,1.0,1.0,86.0,1.0,1113.0,262.0,0.0,0.183206107,0.076923077]),\n",
       " LabeledPoint(1.0, [0.813196,2.128571429,0.476190476,0.180952381,0.047619048,0.023809524,0.52530541,0.0,0.0,0.013615734,0.0,0.174186887,0.611111111,1.0,1.0,59.0,1.0,1029.0,210.0,0.0,0.523809524,0.065217391]),\n",
       " LabeledPoint(1.0, [0.330629,1.661016949,0.243697479,0.084033613,0.033613445,0.033613445,0.460307499,0.0,0.0,0.007462687,0.0,0.271262751,0.051482059,0.0,1.0,6.0,0.0,14182.0,119.0,2.0,0.025210084,0.072368421]),\n",
       " LabeledPoint(0.0, [0.437275,0.897959184,0.166666667,0.075757576,0.050505051,0.03030303,0.417047184,0.0,0.0,0.057491289,0.0,0.144682597,1.223214286,1.0,1.0,64.0,0.0,562.0,198.0,4.0,0.035353535,0.151260504]),\n",
       " LabeledPoint(1.0, [0.85,2.662337662,0.587096774,0.258064516,0.161290323,0.122580645,0.446401458,0.0,0.0,0.049032258,0.0,0.244977665,0.031100478,1.0,1.0,16.0,0.0,9671.0,155.0,8.0,0.180645161,0.131979695]),\n",
       " LabeledPoint(1.0, [0.673412,1.91521197,0.566585956,0.191283293,0.065375303,0.031476998,0.423876652,0.0,0.0,0.040752351,0.0,0.221514644,0.018788163,1.0,1.0,24.0,0.0,10403.0,413.0,13.0,0.072639225,0.07778865]),\n",
       " LabeledPoint(1.0, [0.0,1.127516779,0.636363636,0.048484848,0.0,0.0,0.534351145,0.0,0.0,0.047830923,0.0,0.30860043,0.368421053,1.0,0.0,15.0,0.0,4372.0,165.0,8.0,0.096969697,0.078431373]),\n",
       " LabeledPoint(1.0, [0.0,2.334975369,0.638132296,0.389105058,0.19844358,0.085603113,0.542857143,0.007407407,0.0,0.027705628,0.0,0.19327272,0.466666667,1.0,1.0,35.0,0.0,3824.0,257.0,3.0,0.26848249,0.107913669]),\n",
       " LabeledPoint(0.0, [0.0,3.731182796,0.510638298,0.244680851,0.042553191,0.021276596,0.456953642,0.0,0.0,0.090573013,0.0,0.277868363,0.431818182,0.0,1.0,72.0,0.0,915.0,94.0,0.0,0.531914894,0.007092199]),\n",
       " LabeledPoint(1.0, [0.344041,2.053763441,0.771573604,0.385786802,0.329949239,0.30964467,0.50287687,0.0,0.0,0.038765823,0.0,0.17601314,0.271084337,1.0,1.0,14.0,0.0,10513.0,197.0,4.0,0.329949239,0.08]),\n",
       " LabeledPoint(0.0, [0.584806,2.514851485,0.5,0.253968254,0.142857143,0.063492063,0.433668801,0.0,0.0,0.024596464,0.0,0.228811123,0.478494624,1.0,1.0,7.0,0.0,17082.0,126.0,7.0,0.047619048,0.099502488]),\n",
       " LabeledPoint(1.0, [0.0,2.336206897,0.526315789,0.210526316,0.090225564,0.037593985,0.545300593,0.0,0.0,0.046511628,0.0,0.208496341,0.631313131,0.0,1.0,18.0,0.0,5472.0,133.0,7.0,0.323308271,0.048543689]),\n",
       " LabeledPoint(0.0, [0.876315,3.81420765,0.58974359,0.261538462,0.179487179,0.107692308,0.452855972,0.0,0.0,0.029478458,0.0,0.18617634,0.059485531,0.0,1.0,45.0,0.0,4370.0,195.0,9.0,0.112820513,0.106180666]),\n",
       " LabeledPoint(0.0, [0.0,3.265306122,0.636363636,0.404040404,0.262626263,0.121212121,0.468955651,0.0,0.0,0.043478261,0.0,0.234063561,0.094720497,1.0,1.0,14.0,0.0,8663.0,99.0,7.0,0.050505051,0.093097913]),\n",
       " LabeledPoint(1.0, [0.991382,1.722222222,0.252336449,0.046728972,0.009345794,0.0,0.62283737,0.0,0.0,0.052873563,0.0,0.263929136,0.221052632,0.0,0.0,49.0,0.0,879.0,107.0,12.0,0.224299065,0.068376068]),\n",
       " LabeledPoint(1.0, [0.747563,2.872881356,0.591666667,0.266666667,0.133333333,0.075,0.0,-1.0,0.0,0.065957447,0.0,0.265870884,-1.0,1.0,1.0,30.0,0.0,3868.0,120.0,7.0,0.083333333,0.153846154]),\n",
       " LabeledPoint(1.0, [0.625311,1.852272727,0.336633663,0.217821782,0.158415842,0.128712871,0.414051427,0.0,0.0,0.053359684,0.0,0.252102467,0.045989305,1.0,1.0,11.0,0.0,5431.0,101.0,4.0,0.01980198,0.106164384]),\n",
       " LabeledPoint(1.0, [0.497692,3.686567164,0.519230769,0.403846154,0.365384615,0.0,0.480106101,0.0,0.0,0.021961933,0.0,0.228712429,0.017725258,1.0,0.0,7.0,0.0,14364.0,104.0,4.0,0.423076923,0.109004739]),\n",
       " LabeledPoint(1.0, [0.884415,1.447674419,0.786096257,0.106951872,0.032085561,0.005347594,0.490751227,0.0,0.0,0.060371517,0.0,0.238531699,0.03030303,1.0,1.0,28.0,0.0,2853.0,187.0,8.0,0.058823529,0.115618661]),\n",
       " LabeledPoint(1.0, [0.707823,2.776978417,0.446666667,0.32,0.186666667,0.173333333,0.468774094,0.0,0.0,0.038523274,0.0,0.248972737,0.140625,0.0,1.0,22.0,0.0,6560.0,150.0,5.0,0.033333333,0.082758621]),\n",
       " LabeledPoint(1.0, [0.0,2.230769231,0.658536585,0.317073171,0.243902439,0.170731707,21.0,-1.0,0.0,0.022304833,0.0,0.234148344,-1.0,0.0,0.0,28.0,0.0,1000.0,41.0,0.0,0.780487805,0.090909091]),\n",
       " LabeledPoint(0.0, [0.633821,2.663716814,0.421875,0.1796875,0.109375,0.0859375,0.600694444,0.0,0.0,0.073883162,0.0,0.211271867,0.215277778,0.0,1.0,34.0,0.0,2919.0,128.0,9.0,0.609375,0.102564103]),\n",
       " LabeledPoint(1.0, [0.0,2.531914894,0.612244898,0.285714286,0.183673469,0.06122449,0.621908127,0.013513514,0.0,0.016528926,0.0,0.294793449,0.121621622,0.0,1.0,14.0,0.0,3871.0,49.0,0.0,0.040816327,0.117647059]),\n",
       " LabeledPoint(1.0, [0.0,1.268041237,0.336134454,0.06302521,0.016806723,0.012605042,0.471284372,0.0,0.0,0.034980989,0.0,0.175704022,0.17794971,1.0,1.0,11.0,0.0,9499.0,238.0,5.0,0.399159664,0.067864271]),\n",
       " LabeledPoint(1.0, [0.572142,0.934065934,0.304347826,0.108695652,0.010869565,0.0,0.725146199,0.0,0.0,0.076923077,0.0,0.169670508,2.16,1.0,1.0,57.0,0.0,343.0,92.0,3.0,0.02173913,0.088235294]),\n",
       " LabeledPoint(1.0, [0.576678,1.268115942,0.436619718,0.154929577,0.126760563,0.112676056,0.334907889,0.0,0.0,0.036297641,0.0,0.223623753,0.065454545,0.0,0.0,9.0,0.0,7989.0,142.0,4.0,0.056338028,0.043636364]),\n",
       " LabeledPoint(0.0, [0.810992,2.96039604,0.58490566,0.377358491,0.094339623,0.047169811,0.7960199,0.0,0.0,0.036199095,0.0,0.257144005,0.675675676,1.0,1.0,59.0,0.0,1064.0,106.0,2.0,0.179245283,0.03030303]),\n",
       " LabeledPoint(0.0, [0.368871,4.066666667,0.408759124,0.02189781,0.02189781,0.0,0.636363636,0.0,0.0,0.00754717,0.0,0.166403287,0.985915493,0.0,0.0,37.0,0.0,1177.0,137.0,3.0,0.751824818,0.094594595]),\n",
       " LabeledPoint(0.0, [0.888233,0.530864198,0.14084507,0.084507042,0.0,0.0,0.488843813,0.0,0.0,0.029622063,0.0,0.192870423,0.384180791,1.0,0.0,14.0,0.0,2650.0,213.0,5.0,0.422535211,0.083333333]),\n",
       " LabeledPoint(1.0, [0.313084,1.633663366,0.417475728,0.174757282,0.155339806,0.067961165,0.399350029,0.0,0.0,0.044742729,0.0,0.248999144,0.020742358,1.0,0.0,13.0,0.0,5190.0,103.0,1.0,0.13592233,0.081585082]),\n",
       " LabeledPoint(0.0, [0.0,1.40433213,0.43772242,0.1886121,0.03202847,0.021352313,0.483477377,0.0,0.0,0.035330261,0.0,0.174798117,0.151515152,1.0,1.0,29.0,1.0,4742.0,281.0,0.0,0.241992883,0.099071207]),\n",
       " LabeledPoint(1.0, [0.695459,1.962264151,0.457627119,0.305084746,0.016949153,0.0,0.446292446,0.0,0.0,0.102362205,0.0,0.278671409,0.008805031,1.0,0.0,12.0,0.0,3393.0,59.0,6.0,0.06779661,0.082397004]),\n",
       " LabeledPoint(1.0, [0.0,2.557471264,0.608247423,0.268041237,0.077319588,0.030927835,21.0,-1.0,0.0,0.061020036,0.0,0.25836663,-1.0,1.0,1.0,16.0,0.0,11480.0,194.0,7.0,0.278350515,0.090909091]),\n",
       " LabeledPoint(1.0, [0.584688,2.2,0.506666667,0.206666667,0.066666667,0.026666667,0.359460445,0.0,0.0,0.059282371,0.0,0.236588977,0.074162679,1.0,1.0,31.0,0.0,3453.0,150.0,9.0,0.26,0.21182266]),\n",
       " LabeledPoint(0.0, [0.849737,3.266666667,0.333333333,0.066666667,0.0,0.0,0.618934911,0.0,0.0,0.024390244,0.0,0.450518135,0.0,0.0,1.0,21.0,0.0,940.0,15.0,0.0,0.0,0.096296296]),\n",
       " LabeledPoint(0.0, [0.729716,0.972222222,0.128205128,0.025641026,0.0,0.0,0.595588235,0.0,0.0,0.014534884,0.0,0.218373437,1.125,1.0,0.0,61.0,0.0,423.0,117.0,6.0,0.017094017,0.102941176]),\n",
       " LabeledPoint(1.0, [0.777652,1.669354839,0.337931034,0.082758621,0.006896552,0.0,0.462476294,0.0,0.0,0.05862069,1.0,0.33413631,0.03196347,1.0,0.0,23.0,0.0,3274.0,145.0,6.0,0.027586207,0.074303406]),\n",
       " LabeledPoint(1.0, [0.449769,2.516853933,0.447916667,0.15625,0.083333333,0.010416667,0.896,0.0,0.0,0.058426966,0.0,0.221822252,0.666666667,0.0,1.0,33.0,0.0,2100.0,96.0,4.0,0.0625,0.033333333]),\n",
       " LabeledPoint(0.0, [0.0,3.596774194,0.384615385,0.107692308,0.092307692,0.061538462,0.473053892,0.0,0.0,0.065296252,0.0,0.221687874,0.033591731,0.0,1.0,7.0,0.0,13636.0,65.0,8.0,0.092307692,0.089005236]),\n",
       " LabeledPoint(0.0, [0.0784091,1.545454545,0.345454545,0.163636364,0.0,0.0,21.0,-1.0,0.0,0.021875,0.0,0.27943686,-1.0,1.0,0.0,45.0,1.0,456.0,55.0,0.0,0.836363636,0.25]),\n",
       " LabeledPoint(1.0, [0.0,1.110132159,0.867684478,0.197201018,0.077608142,0.06870229,0.678947368,0.0,0.0,0.006795017,0.0,0.147318493,0.295774648,0.0,0.0,51.0,0.0,2780.0,786.0,3.0,0.0,0.12]),\n",
       " LabeledPoint(1.0, [0.895264,3.036363636,0.5,0.205263158,0.110526316,0.042105263,0.40858209,0.0,0.0,0.015732547,0.0,0.224142252,0.780748663,1.0,1.0,8.0,0.0,29601.0,190.0,6.0,0.121052632,0.065326633])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 报错No module named 'numpy' 使用sudo pip3 install numpy安装到/usr/local/lib/python3.5/dist-packages/numpy而不是用pip3 install numpy安装到/home/zh/.local/lib/python3.5/site-packages/numpy      \n",
    "* 虽然权限由zh:zh变为了root:stuff，但是却能够使用了，成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cache()\n",
    "data.checkpoint()\n",
    "nbdata.cache()\n",
    "nbdata.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "lrModel = LogisticRegressionWithSGD.train(data, 10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel= SVMWithSGD.train(data, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel = NaiveBayes.train(nbdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 走到这里总要删除在/home/zh/Spark_ML_Experiment/metastore_db中的db.lck和dbex.lck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel = DecisionTree.trainClassifier(data=data,numClasses=2,categoricalFeaturesInfo={}, impurity='gini', maxDepth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_point = data.first()\n",
    "first_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.predict(first_point.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrTotalCorrect = data.map(lambda x:1 if lrModel.predict(x.features) == x.label else 0).reduce(lambda x, y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5146720757268425"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrTotalCorrect/ data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 即使在训练集，其准确率(而不是AUC)也差的可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmTotalCorrect = data.map(lambda x:1 if svmModel.predict(x.features) == x.label else 0).reduce(lambda x, y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5146720757268425"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmTotalCorrect/ data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 794, in _batch_appends\n",
      "    save(x)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 841, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/context.py\", line 306, in __getnewargs__\n",
      "    \"It appears that you are attempting to reference SparkContext from a broadcast \"\n",
      "Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_skel_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPENDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_skel_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m#print(\"save global\", islambda(obj), obj.__code__.co_filename, modname, themodule)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# save the rest of the func data needed by _fill_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         raise Exception(\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;34m\"It appears that you are attempting to reference SparkContext from a broadcast \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;34m\"variable, action, or transformation. SparkContext can only be used on the driver, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ea4afcd90d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtTotalCorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2455\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2456\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2457\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2386\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2390\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2372\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_memoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "dtTotalCorrect = data.map(lambda x:(1 if (dtModel.predict(x.features) >0.5)==x.label else 0)).reduce(lambda x, y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 决策树选练出来的模型在预测的时候好像会引用SparkContext对象，而这个对象只能在driver节点出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPredictions = dtModel.predict(data.map(lambda x:x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtPredictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predict = data.map(lambda x:x.label).zip(dtPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* zip方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 1.0),\n",
       " (1.0, 1.0),\n",
       " (1.0, 1.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6482758620689655"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict.map(lambda x:1 if x[0]==x[1] else 0).sum() / data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC和AUC面积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_binary_classification_evaluation(model, data):\n",
    "    return BinaryClassificationMetrics(data.map(lambda x: (float(model.predict(x.features)), x.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_b_e = model_binary_classification_evaluation(lrModel, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5014181143280931"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_b_e.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567586293858841"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_b_e.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_b_e = model_binary_classification_evaluation(svmModel, nbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567586293858841"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_b_e.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5014181143280931"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_b_e.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_b_e = model_binary_classification_evaluation(nbModel, nbdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6808510815151734"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_b_e.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5835585110136261"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_b_e.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_b_e = BinaryClassificationMetrics(label_predict.map(lambda x:(x[1],x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7428940542013553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_b_e.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6489164974113228"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_b_e.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进模型性能和参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = data.map(lambda x:x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = RowMatrix(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixSummary = matrix.computeColumnSummaryStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.12258053e-01,   2.76182319e+00,   4.68230473e-01,\n",
       "         2.14079926e-01,   9.20623607e-02,   4.92621604e-02,\n",
       "         2.25510345e+00,  -1.03750428e-01,   0.00000000e+00,\n",
       "         5.64227450e-02,   2.12305612e-02,   2.33778177e-01,\n",
       "         2.75709037e-01,   6.15551048e-01,   6.60311021e-01,\n",
       "         3.00770791e+01,   3.97565923e-02,   5.71659824e+03,\n",
       "         1.78754564e+02,   4.96064909e+00,   1.72864050e-01,\n",
       "         1.01220792e-01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrixSummary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5053.,  7354.,  7172.,  6821.,  6160.,  5128.,  7350.,  1257.,\n",
       "           0.,  7362.,   157.,  7395.,  7355.,  4552.,  4883.,  7347.,\n",
       "         294.,  7378.,  7395.,  6782.,  6868.,  7235.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrixSummary.numNonzeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True).fit(vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = data.map(lambda x:LabeledPoint(x.label, scaler.transform(x.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 600, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/context.py\", line 306, in __getnewargs__\n",
      "    \"It appears that you are attempting to reference SparkContext from a broadcast \"\n",
      "Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_skel_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m#print(\"save global\", islambda(obj), obj.__code__.co_filename, modname, themodule)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# save the rest of the func data needed by _fill_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         raise Exception(\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;34m\"It appears that you are attempting to reference SparkContext from a broadcast \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;34m\"variable, action, or transformation. SparkContext can only be used on the driver, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getnewargs__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2455\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2456\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2457\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2386\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2390\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2372\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_memoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "scale_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* scaler好像会引用SparkContext对象，而这个对象只能在driver节点出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.map(lambda x:x.label)\n",
    "features = data.map(lambda x:x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = labels.zip(scaler.transform(features)).map(lambda x:LabeledPoint(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without trying to give a complete list, map, filter, flatMap, and coalesce (with shuffle=false) do preserve the order. sortBy, partitionBy, join do not preserve the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [1.1376473365,-0.0819355716929,1.02513981289,-0.0558635644254,-0.468893253129,-0.354305326308,-0.317535217236,0.33845079824,0.0,0.828822173315,-0.147268943346,0.229639823578,-0.141625969099,0.790238049918,0.717194729453,-0.297996816496,-0.20346257793,-0.0329672096969,-0.0487811297558,0.940069975117,-0.108698488525,-0.278820782314])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "lr_scale = LogisticRegressionWithSGD.train(scale_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scale_b_e = model_binary_classification_evaluation(lr_scale, scale_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7277006868096805"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scale_b_e.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6201898373011353"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scale_b_e.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = records.map(lambda x:x[3]).distinct().zipWithIndex().map(\n",
    "    lambda x:(x[0].replace('\\\"', \"\"), x[1])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'?': 10,\n",
       " 'arts_entertainment': 12,\n",
       " 'business': 8,\n",
       " 'computer_internet': 5,\n",
       " 'culture_politics': 9,\n",
       " 'gaming': 7,\n",
       " 'health': 1,\n",
       " 'law_crime': 13,\n",
       " 'recreation': 4,\n",
       " 'religion': 3,\n",
       " 'science_technology': 0,\n",
       " 'sports': 2,\n",
       " 'unknown': 6,\n",
       " 'weather': 11}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_one_hot(_index, _max_index):\n",
    "    _list = [0]*(_max_index+1)\n",
    "    _list[_index] = 1\n",
    "    return _list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map_category = records.map(\n",
    "    lambda a_list:[x.replace('\\\"', '') for x in a_list]\n",
    "    ).map(\n",
    "    lambda x:x[3:]\n",
    "    ).map( \n",
    "    lambda x:return_one_hot(categories.get(x[0]), 13)+x[1:]\n",
    "    ).map(\n",
    "    lambda a_list:([(0.0 if i=='?' else float(i)) for i in a_list[:-1] ] +[int(a_list[-1]), ])\n",
    "    ).map(\n",
    "    lambda x:LabeledPoint(x[-1], x[:-1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map_category.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map_category.cache()\n",
    "data_map_category.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将onehot变量不归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_af14 = data_map_category.map(lambda x:x.features[14:])\n",
    "vectors_be14 = data_map_category.map(lambda x:x.features[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_map_category.map(lambda x:x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True).fit(vectors_af14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/feature.py:237: UserWarning: Both withMean and withStd are false. The model does nothing.\n",
      "  warnings.warn(\"Both withMean and withStd are false. The model does nothing.\")\n"
     ]
    }
   ],
   "source": [
    "non_scaler = StandardScaler(withMean=False, withStd=False).fit(vectors_be14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_af14_transformed = scaler.transform(vectors_af14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_be14_transformed = non_scaler.transform(vectors_be14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " DenseVector([0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
       " DenseVector([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_be14_transformed.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.1376, -0.0819, 1.0251, -0.0559, -0.4689, -0.3543, -0.3175, 0.3385, 0.0, 0.8288, -0.1473, 0.2296, -0.1416, 0.7902, 0.7172, -0.298, -0.2035, -0.033, -0.0488, 0.9401, -0.1087, -0.2788]),\n",
       " DenseVector([0.4887, 0.1063, 0.1959, 0.509, 1.2695, 1.3097, -0.3132, 0.3385, 0.0, 1.0202, -0.1473, -0.5771, -0.0975, 0.7902, 0.7172, 0.4866, -0.2035, -0.0838, 0.0459, 1.2494, 0.0489, 0.3058]),\n",
       " DenseVector([1.7637, -0.044, 0.4617, 0.7334, 0.2927, -0.0912, -0.3032, 0.3385, 0.0, 0.3867, -0.1473, -0.1405, -0.0808, 0.7902, 0.7172, 1.2221, -0.2035, -0.3917, 0.4416, 1.868, -0.0338, -0.5504])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_af14_transformed.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_category = labels.zip(\n",
    "    vectors_be14_transformed.zip(vectors_af14_transformed)\n",
    "    ).map(\n",
    "    lambda x:LabeledPoint(label=x[0], features=list(x[1][0])+list(x[1][1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.1376473365,-0.0819355716929,1.02513981289,-0.0558635644254,-0.468893253129,-0.354305326308,-0.317535217236,0.33845079824,0.0,0.828822173315,-0.147268943346,0.229639823578,-0.141625969099,0.790238049918,0.717194729453,-0.297996816496,-0.20346257793,-0.0329672096969,-0.0487811297558,0.940069975117,-0.108698488525,-0.278820782314]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.488685990417,0.106283637051,0.195885662909,0.508986806825,1.26946916328,1.30971389846,-0.313176090577,0.33845079824,0.0,1.02024383053,-0.147268943346,-0.577072420563,-0.0974598108014,0.790238049918,0.717194729453,0.486582251769,-0.20346257793,-0.0837816352001,0.0459442290216,1.24936955983,0.0488534204631,0.305780221901]),\n",
       " LabeledPoint(1.0, [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.76370015145,-0.0439616503324,0.461691874163,0.733429729796,0.292698491466,-0.0912380098151,-0.303218882637,0.33845079824,0.0,0.386653800111,-0.147268943346,-0.140538950608,-0.0808480767065,0.790238049918,0.717194729453,1.22212512827,-0.20346257793,-0.391710293516,0.441561903916,1.86796872925,-0.0338127019291,-0.550386680318])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final_category.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_category.cache()\n",
    "data_final_category.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "lrModel_final_category = LogisticRegressionWithSGD.train(data_final_category, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights=[-0.00792574411643,0.0221284240296,-0.0685911984591,-0.00260629219482,0.135510393775,-0.0453595281002,-0.00026715387149,-0.00525769982276,0.111543189238,-0.0098876894717,-0.00393372596453,-0.00101315418085,-0.0715674385729,-0.000864044215895,-0.0396835810296,0.00119780204597,0.0559243709885,0.0454526647813,0.0752761465863,0.0341963417916,-0.0365815338359,0.00409783700074,0.0,-0.204619266607,0.000369250089485,-0.0182314776664,-0.0350760636488,0.0065031576843,-0.0155958793842,-0.253625708186,-0.0505410845545,0.0361040976664,0.0566863603119,-0.0554656951726,0.0021496685791,-0.0693723129283], intercept=0.0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel_final_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel_category_result = model_binary_classification_evaluation(lrModel_final_category, data_final_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7458593734287806"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel_category_result.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.641908821119085"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel_category_result.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将onehot变量和其他变量同时归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_map_category.map(lambda x:x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_map_category.map(lambda x:x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_standarded = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_category = labels.zip(features_standarded).map(lambda x:LabeledPoint(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [-0.201654052319,-0.270999069693,-0.232727977095,-0.0991499193088,-0.446421204794,-0.204182210579,-0.028494000387,-0.101894690972,2.72073665645,-0.220526884579,-0.680752790425,-0.0232621058984,-0.381813223243,-0.0648775723926,1.1376473365,-0.0819355716929,1.02513981289,-0.0558635644254,-0.468893253129,-0.354305326308,-0.317535217236,0.33845079824,0.0,0.828822173315,-0.147268943346,0.229639823578,-0.141625969099,0.790238049918,0.717194729453,-0.297996816496,-0.20346257793,-0.0329672096969,-0.0487811297558,0.940069975117,-0.108698488525,-0.278820782314]),\n",
       " LabeledPoint(1.0, [-0.201654052319,-0.270999069693,-0.232727977095,-0.0991499193088,2.23973405107,-0.204182210579,-0.028494000387,-0.101894690972,-0.367497813919,-0.220526884579,-0.680752790425,-0.0232621058984,-0.381813223243,-0.0648775723926,0.488685990417,0.106283637051,0.195885662909,0.508986806825,1.26946916328,1.30971389846,-0.313176090577,0.33845079824,0.0,1.02024383053,-0.147268943346,-0.577072420563,-0.0974598108014,0.790238049918,0.717194729453,0.486582251769,-0.20346257793,-0.0837816352001,0.0459442290216,1.24936955983,0.0488534204631,0.305780221901])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_category.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "lrModel_category = LogisticRegressionWithSGD.train(data_category, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel_category_result = model_binary_classification_evaluation(lrModel_category, data_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7579822657619434"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel_category_result.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.665475474542015"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel_category_result.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* one-hot变量归一化不是好的实践，会丧失one-hot变量的解释性（不再是简单的0-1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对模型使用正确的数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]),\n",
       " LabeledPoint(1.0, [0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.574147,3.677966102,0.50802139,0.288770053,0.213903743,0.144385027,0.468648998,0.0,0.0,0.098707403,0.0,0.203489628,0.088652482,1.0,1.0,40.0,0.0,4973.0,187.0,9.0,0.181818182,0.125448029])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map_category.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "first14_ = data_map_category.map(lambda x:LabeledPoint(x.label, x.features[0:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first14_.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel_f14 = NaiveBayes.train(first14_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel_category_result = model_binary_classification_evaluation(nbModel_f14, first14_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7405222106704076"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbModel_category_result.areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6051384941549446"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbModel_category_result.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1、PR从68%到74%，ROC从58%到60%\n",
    "* 2、发现不使用后面的float类型变量，仅仅使用一个分类变量的效果就变得更好，强烈说明朴素贝叶斯主要依靠变量出现的频率来推算概率。因为新来的float变量几乎从未出现过，所以效果不好。可以考虑将float变量进行分组（浓缩），应该能提高模型效果。如果float变量有一定的分布，那么可以根据分布得出的概率密度来作为该数值出现的概率\n",
    "* 3、PR的结果不稳定，不太适合作为模型效果参考标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1、线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cache()\n",
    "data.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = sc.parallelize([1,5,10,50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 148, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 740, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 255, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 292, in save_function_tuple\n",
      "    save((code, closure, base_globals))\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 249, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/cloudpickle.py\", line 297, in save_function_tuple\n",
      "    save(f_globals)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/home/zh/spark_hadoop/spark/python/pyspark/rdd.py\", line 204, in __getnewargs__\n",
      "    \"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\n",
      "Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_skel_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_appends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPPEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m#print(\"save global\", islambda(obj), obj.__code__.co_filename, modname, themodule)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# save the rest of the func data needed by _fill_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise Exception(\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;34m\"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;34m\"action or transformation. RDD transformations and actions can only be invoked by the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getnewargs__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2455\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2456\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2457\u001b[0m                                              self.preservesPartitioning)\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2386\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2390\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2372\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark_hadoop/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_memoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: Exception: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "num_iterations.map(lambda x:model_binary_classification_evaluation(LogisticRegressionWithSGD.train(data, x, ), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = [1,5,10,50,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [-0.201654052319,-0.270999069693,-0.232727977095,-0.0991499193088,-0.446421204794,-0.204182210579,-0.028494000387,-0.101894690972,2.72073665645,-0.220526884579,-0.680752790425,-0.0232621058984,-0.381813223243,-0.0648775723926,1.1376473365,-0.0819355716929,1.02513981289,-0.0558635644254,-0.468893253129,-0.354305326308,-0.317535217236,0.33845079824,0.0,0.828822173315,-0.147268943346,0.229639823578,-0.141625969099,0.790238049918,0.717194729453,-0.297996816496,-0.20346257793,-0.0329672096969,-0.0487811297558,0.940069975117,-0.108698488525,-0.278820782314])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_category.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_map_category.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 num_iterations: AUC = 0.6495198950299683\n",
      "5 num_iterations: AUC = 0.6661609623443581\n",
      "10 num_iterations: AUC = 0.665475474542015\n",
      "50 num_iterations: AUC = 0.6673594497476322\n",
      "100 num_iterations: AUC = 0.668377141115478\n"
     ]
    }
   ],
   "source": [
    "for i in num_iterations:\n",
    "    _result = model_binary_classification_evaluation(LogisticRegressionWithSGD.train(data_category, i, ), data_category)\n",
    "    print(\"%s num_iterations: AUC = %s\"%(i, _result.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [0.001,0.01,0.1,1,10,30,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 num_iterations: AUC = 0.6496588225098238\n",
      "0.01 num_iterations: AUC = 0.6496444027450547\n",
      "0.1 num_iterations: AUC = 0.6628247631132755\n",
      "1 num_iterations: AUC = 0.668377141115478\n",
      "10 num_iterations: AUC = 0.6704485732630918\n",
      "30 num_iterations: AUC = 0.6125915362275728\n",
      "100 num_iterations: AUC = 0.5725845942453865\n"
     ]
    }
   ],
   "source": [
    "for i in steps:\n",
    "    _result = model_binary_classification_evaluation(LogisticRegressionWithSGD.train(data_category, step=i, ), data_category)\n",
    "    print(\"%s num_iterations: AUC = %s\"%(i, _result.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "regParams = ['l1', 'l2', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zh/spark_hadoop/spark/python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 num_iterations: AUC = 0.6632637241758116\n",
      "l2 num_iterations: AUC = 0.668377141115478\n"
     ]
    }
   ],
   "source": [
    "for i in regParams:\n",
    "    _result = model_binary_classification_evaluation(LogisticRegressionWithSGD.train(data_category, regType=i ), data_category)\n",
    "    print(\"%s num_iterations: AUC = %s\"%(i, _result.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2、决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3、朴素贝叶斯 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lambda参数控制additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4、交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 跑了一遍简单的模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
